# XMatchAD

The remarkable success of reconstruction-based methods in unsupervised anomaly detection stems from their ability to identify and localize anomalies by modeling the discrepancies between input images and their reconstructed counterparts. However, these approaches often struggle to capture subtle anomalies and may suffer from blurred anomaly boundaries, ultimately limiting their effectiveness, which is particularly pronounced in complex multi-class scenarios. These challenges motivate the development of XMatchAD, a novel anomaly detection framework that reinterprets the task from a pseudo cross-modal matching perspective. Specifically, the input and reconstructed images are treated as two complementary modalities and their matching relationships are precisely exploited for anomaly detection. First, a pre-trained feature extractor is employed to encode discriminative representations. Second, an attention-guided cross-modal matching mechanism is introduced to match local inter-modal anomaly-related patterns while mutually refining the features. This enhances the sensitivity to anomalies with diverse shapes and subtle deviations and significantly improves the precision of anomaly detection and localization. Third, we design an adaptive frequency-aware fusion module that further delineates sharp anomaly boundaries through the coupling of high-frequency components from cross-modal multi-scale representations. Comprehensive evaluations on MVTec-AD, VisA, and MPDD benchmarks demonstrate that our method consistently achieves superior performance, outperforming state-of-the-art methods in multi-class anomaly detection and localization.
